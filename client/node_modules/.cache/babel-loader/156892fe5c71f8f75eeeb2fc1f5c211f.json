{"ast":null,"code":"import _slicedToArray from \"C:\\\\Users\\\\thuyv\\\\Desktop\\\\Work\\\\UTSA\\\\Senior Design\\\\Synthesizer\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/slicedToArray\";\nimport _regeneratorRuntime from \"C:\\\\Users\\\\thuyv\\\\Desktop\\\\Work\\\\UTSA\\\\Senior Design\\\\Synthesizer\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/regenerator\";\nimport _defineProperty from \"C:\\\\Users\\\\thuyv\\\\Desktop\\\\Work\\\\UTSA\\\\Senior Design\\\\Synthesizer\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/defineProperty\";\nimport _objectSpread from \"C:\\\\Users\\\\thuyv\\\\Desktop\\\\Work\\\\UTSA\\\\Senior Design\\\\Synthesizer\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/objectSpread\";\nimport _asyncToGenerator from \"C:\\\\Users\\\\thuyv\\\\Desktop\\\\Work\\\\UTSA\\\\Senior Design\\\\Synthesizer\\\\node_modules\\\\babel-preset-react-app\\\\node_modules\\\\@babel\\\\runtime/helpers/esm/asyncToGenerator\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it = typeof Symbol !== \"undefined\" && o[Symbol.iterator] || o[\"@@iterator\"]; if (!it) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = it.call(o); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\nimport { copyFromChannel } from '../helpers/copy-from-channel';\nimport { copyToChannel } from '../helpers/copy-to-channel';\nimport { createNestedArrays } from '../helpers/create-nested-arrays';\nimport { getAudioNodeConnections } from '../helpers/get-audio-node-connections';\nimport { getAudioWorkletProcessor } from '../helpers/get-audio-worklet-processor';\nimport { isOwnedByContext } from '../helpers/is-owned-by-context';\n\nvar processBuffer = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(proxy, renderedBuffer, nativeOfflineAudioContext, options, outputChannelCount, processorConstructor, exposeCurrentFrameAndCurrentTime) {\n    var length, numberOfInputChannels, numberOfOutputChannels, processedBuffer, audioNodeConnections, audioWorkletProcessor, inputs, outputs, parameters, _loop, i, _ret;\n\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            // Ceil the length to the next full render quantum.\n            // Bug #17: Safari does not yet expose the length.\n            length = renderedBuffer === null ? Math.ceil(proxy.context.length / 128) * 128 : renderedBuffer.length;\n            numberOfInputChannels = options.channelCount * options.numberOfInputs;\n            numberOfOutputChannels = outputChannelCount.reduce(function (sum, value) {\n              return sum + value;\n            }, 0);\n            processedBuffer = numberOfOutputChannels === 0 ? null : nativeOfflineAudioContext.createBuffer(numberOfOutputChannels, length, nativeOfflineAudioContext.sampleRate);\n\n            if (!(processorConstructor === undefined)) {\n              _context.next = 6;\n              break;\n            }\n\n            throw new Error('Missing the processor constructor.');\n\n          case 6:\n            audioNodeConnections = getAudioNodeConnections(proxy);\n            _context.next = 9;\n            return getAudioWorkletProcessor(nativeOfflineAudioContext, proxy);\n\n          case 9:\n            audioWorkletProcessor = _context.sent;\n            inputs = createNestedArrays(options.numberOfInputs, options.channelCount);\n            outputs = createNestedArrays(options.numberOfOutputs, outputChannelCount);\n            parameters = Array.from(proxy.parameters.keys()).reduce(function (prmtrs, name) {\n              return _objectSpread({}, prmtrs, _defineProperty({}, name, new Float32Array(128)));\n            }, {});\n\n            _loop = function _loop(i) {\n              if (options.numberOfInputs > 0 && renderedBuffer !== null) {\n                for (var j = 0; j < options.numberOfInputs; j += 1) {\n                  for (var k = 0; k < options.channelCount; k += 1) {\n                    copyFromChannel(renderedBuffer, inputs[j], k, k, i);\n                  }\n                }\n              }\n\n              if (processorConstructor.parameterDescriptors !== undefined && renderedBuffer !== null) {\n                processorConstructor.parameterDescriptors.forEach(function (_ref2, index) {\n                  var name = _ref2.name;\n                  copyFromChannel(renderedBuffer, parameters, name, numberOfInputChannels + index, i);\n                });\n              }\n\n              for (var _j = 0; _j < options.numberOfInputs; _j += 1) {\n                for (var _k = 0; _k < outputChannelCount[_j]; _k += 1) {\n                  // The byteLength will be 0 when the ArrayBuffer was transferred.\n                  if (outputs[_j][_k].byteLength === 0) {\n                    outputs[_j][_k] = new Float32Array(128);\n                  }\n                }\n              }\n\n              try {\n                var potentiallyEmptyInputs = inputs.map(function (input, index) {\n                  if (audioNodeConnections.activeInputs[index].size === 0) {\n                    return [];\n                  }\n\n                  return input;\n                });\n                var activeSourceFlag = exposeCurrentFrameAndCurrentTime(i / nativeOfflineAudioContext.sampleRate, nativeOfflineAudioContext.sampleRate, function () {\n                  return audioWorkletProcessor.process(potentiallyEmptyInputs, outputs, parameters);\n                });\n\n                if (processedBuffer !== null) {\n                  for (var _j2 = 0, outputChannelSplitterNodeOutput = 0; _j2 < options.numberOfOutputs; _j2 += 1) {\n                    for (var _k2 = 0; _k2 < outputChannelCount[_j2]; _k2 += 1) {\n                      copyToChannel(processedBuffer, outputs[_j2], _k2, outputChannelSplitterNodeOutput + _k2, i);\n                    }\n\n                    outputChannelSplitterNodeOutput += outputChannelCount[_j2];\n                  }\n                }\n\n                if (!activeSourceFlag) {\n                  return \"break\";\n                }\n              } catch (error) {\n                proxy.dispatchEvent(new ErrorEvent('processorerror', {\n                  colno: error.colno,\n                  filename: error.filename,\n                  lineno: error.lineno,\n                  message: error.message\n                }));\n                return \"break\";\n              }\n            };\n\n            i = 0;\n\n          case 15:\n            if (!(i < length)) {\n              _context.next = 22;\n              break;\n            }\n\n            _ret = _loop(i);\n\n            if (!(_ret === \"break\")) {\n              _context.next = 19;\n              break;\n            }\n\n            return _context.abrupt(\"break\", 22);\n\n          case 19:\n            i += 128;\n            _context.next = 15;\n            break;\n\n          case 22:\n            return _context.abrupt(\"return\", processedBuffer);\n\n          case 23:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee);\n  }));\n\n  return function processBuffer(_x, _x2, _x3, _x4, _x5, _x6, _x7) {\n    return _ref.apply(this, arguments);\n  };\n}();\n\nexport var createAudioWorkletNodeRendererFactory = function createAudioWorkletNodeRendererFactory(connectAudioParam, connectMultipleOutputs, createNativeAudioBufferSourceNode, createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeConstantSourceNode, createNativeGainNode, deleteUnrenderedAudioWorkletNode, disconnectMultipleOutputs, exposeCurrentFrameAndCurrentTime, getNativeAudioNode, nativeAudioWorkletNodeConstructor, nativeOfflineAudioContextConstructor, renderAutomation, renderInputsOfAudioNode, renderNativeOfflineAudioContext) {\n  return function (name, options, processorConstructor) {\n    var renderedNativeAudioNodes = new WeakMap();\n    var processedBufferPromise = null;\n\n    var createAudioNode = /*#__PURE__*/function () {\n      var _ref3 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(proxy, nativeOfflineAudioContext) {\n        var nativeAudioWorkletNode, nativeOutputNodes, nativeAudioWorkletNodeIsOwnedByContext, outputChannelCount, numberOfOutputChannels, outputChannelSplitterNode, outputChannelMergerNodes, i, outputGainNode, _numberOfInputChannels, numberOfParameters, numberOfChannels, renderBuffer, _processedBuffer, audioBufferSourceNode, _nativeOutputNodes, _nativeOutputNodes2, _outputChannelSplitterNode, _outputChannelMergerNodes, _outputGainNode, _i3, outputChannelSplitterNodeOutput, outputChannelMergerNode, j, _iterator2, _step2, _step2$value, nm, audioParam, _iterator3, _step3, _step3$value, _nm, _audioParam;\n\n        return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                nativeAudioWorkletNode = getNativeAudioNode(proxy);\n                nativeOutputNodes = null;\n                nativeAudioWorkletNodeIsOwnedByContext = isOwnedByContext(nativeAudioWorkletNode, nativeOfflineAudioContext);\n                outputChannelCount = Array.isArray(options.outputChannelCount) ? options.outputChannelCount : Array.from(options.outputChannelCount); // Bug #61: Only Chrome, Edge, Firefox & Opera have an implementation of the AudioWorkletNode yet.\n\n                if (nativeAudioWorkletNodeConstructor === null) {\n                  numberOfOutputChannels = outputChannelCount.reduce(function (sum, value) {\n                    return sum + value;\n                  }, 0);\n                  outputChannelSplitterNode = createNativeChannelSplitterNode(nativeOfflineAudioContext, {\n                    channelCount: Math.max(1, numberOfOutputChannels),\n                    channelCountMode: 'explicit',\n                    channelInterpretation: 'discrete',\n                    numberOfOutputs: Math.max(1, numberOfOutputChannels)\n                  });\n                  outputChannelMergerNodes = [];\n\n                  for (i = 0; i < proxy.numberOfOutputs; i += 1) {\n                    outputChannelMergerNodes.push(createNativeChannelMergerNode(nativeOfflineAudioContext, {\n                      channelCount: 1,\n                      channelCountMode: 'explicit',\n                      channelInterpretation: 'speakers',\n                      numberOfInputs: outputChannelCount[i]\n                    }));\n                  }\n\n                  outputGainNode = createNativeGainNode(nativeOfflineAudioContext, {\n                    channelCount: options.channelCount,\n                    channelCountMode: options.channelCountMode,\n                    channelInterpretation: options.channelInterpretation,\n                    gain: 1\n                  });\n                  outputGainNode.connect = connectMultipleOutputs.bind(null, outputChannelMergerNodes);\n                  outputGainNode.disconnect = disconnectMultipleOutputs.bind(null, outputChannelMergerNodes);\n                  nativeOutputNodes = [outputChannelSplitterNode, outputChannelMergerNodes, outputGainNode];\n                } else if (!nativeAudioWorkletNodeIsOwnedByContext) {\n                  nativeAudioWorkletNode = new nativeAudioWorkletNodeConstructor(nativeOfflineAudioContext, name);\n                }\n\n                renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeOutputNodes === null ? nativeAudioWorkletNode : nativeOutputNodes[2]);\n\n                if (!(nativeOutputNodes !== null)) {\n                  _context4.next = 41;\n                  break;\n                }\n\n                if (!(processedBufferPromise === null)) {\n                  _context4.next = 32;\n                  break;\n                }\n\n                if (!(processorConstructor === undefined)) {\n                  _context4.next = 10;\n                  break;\n                }\n\n                throw new Error('Missing the processor constructor.');\n\n              case 10:\n                if (!(nativeOfflineAudioContextConstructor === null)) {\n                  _context4.next = 12;\n                  break;\n                }\n\n                throw new Error('Missing the native OfflineAudioContext constructor.');\n\n              case 12:\n                // Bug #47: The AudioDestinationNode in Safari gets not initialized correctly.\n                _numberOfInputChannels = proxy.channelCount * proxy.numberOfInputs;\n                numberOfParameters = processorConstructor.parameterDescriptors === undefined ? 0 : processorConstructor.parameterDescriptors.length;\n                numberOfChannels = _numberOfInputChannels + numberOfParameters;\n\n                renderBuffer = /*#__PURE__*/function () {\n                  var _ref4 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3() {\n                    var partialOfflineAudioContext, gainNodes, inputChannelSplitterNodes, _i, constantSourceNodes, inputChannelMergerNode, _i2, j, _iterator, _step, _step$value, index, constantSourceNode;\n\n                    return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n                      while (1) {\n                        switch (_context3.prev = _context3.next) {\n                          case 0:\n                            partialOfflineAudioContext = new nativeOfflineAudioContextConstructor(numberOfChannels, // Ceil the length to the next full render quantum.\n                            // Bug #17: Safari does not yet expose the length.\n                            Math.ceil(proxy.context.length / 128) * 128, nativeOfflineAudioContext.sampleRate);\n                            gainNodes = [];\n                            inputChannelSplitterNodes = [];\n\n                            for (_i = 0; _i < options.numberOfInputs; _i += 1) {\n                              gainNodes.push(createNativeGainNode(partialOfflineAudioContext, {\n                                channelCount: options.channelCount,\n                                channelCountMode: options.channelCountMode,\n                                channelInterpretation: options.channelInterpretation,\n                                gain: 1\n                              }));\n                              inputChannelSplitterNodes.push(createNativeChannelSplitterNode(partialOfflineAudioContext, {\n                                channelCount: options.channelCount,\n                                channelCountMode: 'explicit',\n                                channelInterpretation: 'discrete',\n                                numberOfOutputs: options.channelCount\n                              }));\n                            }\n\n                            _context3.next = 6;\n                            return Promise.all(Array.from(proxy.parameters.values()).map( /*#__PURE__*/function () {\n                              var _ref5 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(audioParam) {\n                                var constantSourceNode;\n                                return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n                                  while (1) {\n                                    switch (_context2.prev = _context2.next) {\n                                      case 0:\n                                        constantSourceNode = createNativeConstantSourceNode(partialOfflineAudioContext, {\n                                          channelCount: 1,\n                                          channelCountMode: 'explicit',\n                                          channelInterpretation: 'discrete',\n                                          offset: audioParam.value\n                                        });\n                                        _context2.next = 3;\n                                        return renderAutomation(partialOfflineAudioContext, audioParam, constantSourceNode.offset);\n\n                                      case 3:\n                                        return _context2.abrupt(\"return\", constantSourceNode);\n\n                                      case 4:\n                                      case \"end\":\n                                        return _context2.stop();\n                                    }\n                                  }\n                                }, _callee2);\n                              }));\n\n                              return function (_x10) {\n                                return _ref5.apply(this, arguments);\n                              };\n                            }()));\n\n                          case 6:\n                            constantSourceNodes = _context3.sent;\n                            inputChannelMergerNode = createNativeChannelMergerNode(partialOfflineAudioContext, {\n                              channelCount: 1,\n                              channelCountMode: 'explicit',\n                              channelInterpretation: 'speakers',\n                              numberOfInputs: Math.max(1, _numberOfInputChannels + numberOfParameters)\n                            });\n\n                            for (_i2 = 0; _i2 < options.numberOfInputs; _i2 += 1) {\n                              gainNodes[_i2].connect(inputChannelSplitterNodes[_i2]);\n\n                              for (j = 0; j < options.channelCount; j += 1) {\n                                inputChannelSplitterNodes[_i2].connect(inputChannelMergerNode, j, _i2 * options.channelCount + j);\n                              }\n                            }\n\n                            _iterator = _createForOfIteratorHelper(constantSourceNodes.entries());\n\n                            try {\n                              for (_iterator.s(); !(_step = _iterator.n()).done;) {\n                                _step$value = _slicedToArray(_step.value, 2), index = _step$value[0], constantSourceNode = _step$value[1];\n                                constantSourceNode.connect(inputChannelMergerNode, 0, _numberOfInputChannels + index);\n                                constantSourceNode.start(0);\n                              }\n                            } catch (err) {\n                              _iterator.e(err);\n                            } finally {\n                              _iterator.f();\n                            }\n\n                            inputChannelMergerNode.connect(partialOfflineAudioContext.destination);\n                            _context3.next = 14;\n                            return Promise.all(gainNodes.map(function (gainNode) {\n                              return renderInputsOfAudioNode(proxy, partialOfflineAudioContext, gainNode);\n                            }));\n\n                          case 14:\n                            return _context3.abrupt(\"return\", renderNativeOfflineAudioContext(partialOfflineAudioContext));\n\n                          case 15:\n                          case \"end\":\n                            return _context3.stop();\n                        }\n                      }\n                    }, _callee3);\n                  }));\n\n                  return function renderBuffer() {\n                    return _ref4.apply(this, arguments);\n                  };\n                }();\n\n                _context4.t0 = processBuffer;\n                _context4.t1 = proxy;\n\n                if (!(numberOfChannels === 0)) {\n                  _context4.next = 22;\n                  break;\n                }\n\n                _context4.t2 = null;\n                _context4.next = 25;\n                break;\n\n              case 22:\n                _context4.next = 24;\n                return renderBuffer();\n\n              case 24:\n                _context4.t2 = _context4.sent;\n\n              case 25:\n                _context4.t3 = _context4.t2;\n                _context4.t4 = nativeOfflineAudioContext;\n                _context4.t5 = options;\n                _context4.t6 = outputChannelCount;\n                _context4.t7 = processorConstructor;\n                _context4.t8 = exposeCurrentFrameAndCurrentTime;\n                processedBufferPromise = (0, _context4.t0)(_context4.t1, _context4.t3, _context4.t4, _context4.t5, _context4.t6, _context4.t7, _context4.t8);\n\n              case 32:\n                _context4.next = 34;\n                return processedBufferPromise;\n\n              case 34:\n                _processedBuffer = _context4.sent;\n                audioBufferSourceNode = createNativeAudioBufferSourceNode(nativeOfflineAudioContext, {\n                  buffer: null,\n                  channelCount: 2,\n                  channelCountMode: 'max',\n                  channelInterpretation: 'speakers',\n                  loop: false,\n                  loopEnd: 0,\n                  loopStart: 0,\n                  playbackRate: 1\n                });\n                _nativeOutputNodes = nativeOutputNodes, _nativeOutputNodes2 = _slicedToArray(_nativeOutputNodes, 3), _outputChannelSplitterNode = _nativeOutputNodes2[0], _outputChannelMergerNodes = _nativeOutputNodes2[1], _outputGainNode = _nativeOutputNodes2[2];\n\n                if (_processedBuffer !== null) {\n                  audioBufferSourceNode.buffer = _processedBuffer;\n                  audioBufferSourceNode.start(0);\n                }\n\n                audioBufferSourceNode.connect(_outputChannelSplitterNode);\n\n                for (_i3 = 0, outputChannelSplitterNodeOutput = 0; _i3 < proxy.numberOfOutputs; _i3 += 1) {\n                  outputChannelMergerNode = _outputChannelMergerNodes[_i3];\n\n                  for (j = 0; j < outputChannelCount[_i3]; j += 1) {\n                    _outputChannelSplitterNode.connect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);\n                  }\n\n                  outputChannelSplitterNodeOutput += outputChannelCount[_i3];\n                }\n\n                return _context4.abrupt(\"return\", _outputGainNode);\n\n              case 41:\n                if (nativeAudioWorkletNodeIsOwnedByContext) {\n                  _context4.next = 61;\n                  break;\n                }\n\n                _iterator2 = _createForOfIteratorHelper(proxy.parameters.entries());\n                _context4.prev = 43;\n\n                _iterator2.s();\n\n              case 45:\n                if ((_step2 = _iterator2.n()).done) {\n                  _context4.next = 51;\n                  break;\n                }\n\n                _step2$value = _slicedToArray(_step2.value, 2), nm = _step2$value[0], audioParam = _step2$value[1];\n                _context4.next = 49;\n                return renderAutomation(nativeOfflineAudioContext, audioParam, // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.\n                nativeAudioWorkletNode.parameters.get(nm));\n\n              case 49:\n                _context4.next = 45;\n                break;\n\n              case 51:\n                _context4.next = 56;\n                break;\n\n              case 53:\n                _context4.prev = 53;\n                _context4.t9 = _context4[\"catch\"](43);\n\n                _iterator2.e(_context4.t9);\n\n              case 56:\n                _context4.prev = 56;\n\n                _iterator2.f();\n\n                return _context4.finish(56);\n\n              case 59:\n                _context4.next = 78;\n                break;\n\n              case 61:\n                _iterator3 = _createForOfIteratorHelper(proxy.parameters.entries());\n                _context4.prev = 62;\n\n                _iterator3.s();\n\n              case 64:\n                if ((_step3 = _iterator3.n()).done) {\n                  _context4.next = 70;\n                  break;\n                }\n\n                _step3$value = _slicedToArray(_step3.value, 2), _nm = _step3$value[0], _audioParam = _step3$value[1];\n                _context4.next = 68;\n                return connectAudioParam(nativeOfflineAudioContext, _audioParam, // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.\n                nativeAudioWorkletNode.parameters.get(_nm));\n\n              case 68:\n                _context4.next = 64;\n                break;\n\n              case 70:\n                _context4.next = 75;\n                break;\n\n              case 72:\n                _context4.prev = 72;\n                _context4.t10 = _context4[\"catch\"](62);\n\n                _iterator3.e(_context4.t10);\n\n              case 75:\n                _context4.prev = 75;\n\n                _iterator3.f();\n\n                return _context4.finish(75);\n\n              case 78:\n                _context4.next = 80;\n                return renderInputsOfAudioNode(proxy, nativeOfflineAudioContext, nativeAudioWorkletNode);\n\n              case 80:\n                return _context4.abrupt(\"return\", nativeAudioWorkletNode);\n\n              case 81:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, null, [[43, 53, 56, 59], [62, 72, 75, 78]]);\n      }));\n\n      return function createAudioNode(_x8, _x9) {\n        return _ref3.apply(this, arguments);\n      };\n    }();\n\n    return {\n      render: function render(proxy, nativeOfflineAudioContext) {\n        deleteUnrenderedAudioWorkletNode(nativeOfflineAudioContext, proxy);\n        var renderedNativeAudioWorkletNodeOrGainNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);\n\n        if (renderedNativeAudioWorkletNodeOrGainNode !== undefined) {\n          return Promise.resolve(renderedNativeAudioWorkletNodeOrGainNode);\n        }\n\n        return createAudioNode(proxy, nativeOfflineAudioContext);\n      }\n    };\n  };\n};","map":null,"metadata":{},"sourceType":"module"}